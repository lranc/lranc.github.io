<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[6_5173爬虫优化]]></title>
    <url>%2F2018%2F02%2F25%2F6-5173%E7%88%AC%E8%99%AB%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[今天继续前一天写的爬虫,昨晚在爬取到240页的时候,还是因为Ip地址不够而抓取失败了,今天的任务是在删除数据库中的重复数据之后增量抓取.所以文章结构是:1.删除MongoDB中的重复数据2.布隆过滤器增量抓取3.redis初步学习 简单方法在今天的爬取过程中发现,商品的发布时间其实在url里,所以并不需要访问其详情页进去爬取()!!!错误其商品url时间为商品上架时间,而不是商品价格时间的具体表现,而这个爬虫的目的是为了知晓商品的价格走势,故没用 1.删除MongoDB中的重复数据:由于对MongoDB掌握不是很熟练,是在网上找到的方法.[1][][1]http://blog.csdn.net/cloume/article/details/74931998该方法是先将数据导出为json,然后删除collection的数据,再新建唯一index,需要注意的是不能在删除重复数据后直接导入,虽然建立索引后插入数据,得到了一个每个url都唯一的collection,但是在之后的增量爬取时,因为index的升降序问题,导致有的数据无法插入,因此有2种方案(其实一样):a.在建立索引后,导入数据,得到一份无重复的数据,再导出,再remove,增量爬取,最后插入第二次导出的数据b.在建立索引后,导入数据,得到一份无重复的数据,新建一份collection,增量爬取,最后合并数据具体代码:1234567$ mongoexport -d 5173_jw3_price -c jw3_goods -o jw3_goods.json$ mongo$ use 5173_jw3_price #切换数据库$ db.jw3_goods.remove(&#123;&#125;) #删除数据$ db.jw3_goods.createIndex(&#123;goods_url:1&#125;,&#123;unique:true&#125;) #建立以goods_url唯一索引的文档$ exit# mongoimport -d 5173_jw3_price -c jw3_goods --upsert jw3_goods.json #插入数据 2.布隆过滤器增量抓取:对昨天的5173_spider.py文件进行修改,将以下部分以及逻辑判断加入123456789101112131415#首先将所有已抓取的goods_url导出:$ mongoexport -d 5173_jw3_price -c jw3_goods -f goods_url -o jw3_goods_url.json------import jsonimport sysfrom pybloom import ScalableBloomFilter #导入可动态扩展容量的ScalableBloomFilterwith open(&apos;jw3_goods_url.json&apos;,&apos;r&apos;) as f: url_list = [json.loads(x)[&apos;goods_url&apos;] for x in f.readlines()] #获取已抓取的url列表sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)for i in url_list: sbf.add(i)print(sys.getsizeof(sbf)) #56print(sys.getsizeof(url_list)) #61432 3.redis初步学习见下一篇…]]></content>
  </entry>
  <entry>
    <title><![CDATA[5_抓取5173商品信息]]></title>
    <url>%2F2018%2F02%2F24%2F5-%E6%8A%93%E5%8F%965173%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[今天这个爬虫真的是抓的我头都大飞了…初步看这个网站的数据,结构清晰层次合理,完美啊!…没想到有一个大坑坑坑坑坑… 本来继上一篇blog所想,是学习redis分布式的,结果一天都卡在IP代理池的应用…话不多说,首先确定我们需要的item结构,商品url,title,price,info,time,幸运的是5173网站商品结构清晰,从html中十分容易提取 可以看到,在商品列表中每一项商品栏可以提取item的前四项,而第五项,商品上架时间也可以通过提取的url访问新页面,再提取出去 这样计算大概需要369×40+369=15129次访问(14760个商品页+369个商品列表页),即可获取全部数据…然而事与愿违,昨晚将基本代码完成之后,爬取了一页…爬虫便倒下了…昨夜还想着,哇,终于遇到一个封IP的网站了,今天可以好好干一架,便喜滋滋地去睡觉了… 以下是总结今天遇到的几个难点:1).scrapy 使用proxy代理,完全没有用啊!!!肯定是我的使用方式不对!但是就是没找出原因,无论是百度还是知乎还是stackoverflow,都没有找到相应的解决方案,目前几个猜测点: scrapy的版本问题, python3的问题2).好,既然封IP,而自己用scrapy无法实现IP代理,那么就用scrapy.Request获取商品列表页,利用requests.get()获取商品详情页获取time,问题又来了,在不设置time.sleep()的情况下,依然一下子就全封了3).其实也是与上一项相关的,这就令人十分疑惑了,将近100个IP地址一下子全被封,是不是该网站反爬虫的方式不是IP,或者不止于IP?由于自己的知识掌握较少,在使用了随机UA之后,依然没能找出其封禁手段4).没办法,只好在利用requests.get()的同时多sleep下了…目前爬虫仍在运行,不过14760个item却要4个多小时左右完成,确实速度太慢了,而且还不能确保这种抓取方式能否抓完…等待简直煎熬… 改进及构想:1).利用redis分布式抓取,这也是这个项目原本最初的学习目的2).可遍历商品列表,将369×40个商品的url,title,price,info字典格式保存至json,再在之后利用字典中的url增量抓取time,以避免抓取失败,失去进度,顺便进一步学习多线程、多进程、异步以及布隆过滤器,最后保存数据库 最后,希望抓取成功…明天的任务是学习redis,以及mongo删除重复的数据]]></content>
  </entry>
  <entry>
    <title><![CDATA[4_Django初步学习]]></title>
    <url>%2F2018%2F02%2F23%2F4-Django%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[这两天初步学习了下django,照着文档完成了入门教程…学的粗糙,总的来说是一头雾水的… 本来是想着这两天上线一个初步能够使用的查询网站,应用于查询之前所爬取的百度贴吧的骗子信息(未筛选,优化)但没想到django的难度比较大,而马上又要开学找工作了,只好将时间与精力集中与爬虫的学习. 故而在完成文档的入门教程之后,只能在学习爬虫的闲暇之余尝试能不能照着网上可能存在的模板去实现需求 接下来几天的任务:1).爬取5173,争取使用分布式redis完成2).实现一个论坛的抓包分析,及布隆过滤器的增量抓取,先从简单的非scrapy架构3).实现应用scrapy,redis分布式,mongodb的集群以及布隆过滤器抓取,项目待构思4).在前两天,争取粗略实现web查询数据库应用(jw3_cheater_lookup) 加油~]]></content>
  </entry>
  <entry>
    <title><![CDATA[3_剑网3_交易信息网站构思]]></title>
    <url>%2F2018%2F02%2F21%2F3-%E5%89%91%E7%BD%913-%E4%BA%A4%E6%98%93%E4%BF%A1%E6%81%AF%E7%BD%91%E7%AB%99%E6%9E%84%E6%80%9D%2F</url>
    <content type="text"><![CDATA[继上一篇,接下来所要做的工作大体上是建立一个网站,当然,最终目标可不能仅仅提供一个查询骗子信息的接口,还要有其他几部分功能的!2333以下记录下一些想法 建站构思网站主要分为3大功能及几个小功能(目前)1).骗子查询接口,并在后期增加投票功能.2).商品行情信息(价格走势图),并以交易信息发布的多少(发布id判断),模拟其交易量,最终是建立一个期权期货交易平台233333).账号交易信息,交易平台??? 中介中介平台4).骗子举报接口5).评论区???(似乎不重要)6).还没有想出来… 展望嗯,就把建立一个骗子信息查询接口作为建站道路上的第二步吧(第一步是建立此博客hhh)之后的话则想为伟大的百合事业添砖加瓦了!!!这些年看过许多盗版小说也看过正版小说,双方的纠结点大概还是利益吧. 几个小想法:大多数看盗文的应该都追文? 大多数看的是最新章节以及最新完本. 或者说作者的大部分收入来源于正在连载的小说. 是否可以获取作者的同意,便宜出售早期小说,一方面为作者增加流量,并提高当期收益及期望收益,另一方面使得读者以较低的成本获得正版的心理体验以及阅读体验(对于小说以及与其发布网站的版权问题涉及签约合约规定,有待了解) 是否可以建立小说借阅模式的网站或APP(采用阅后即焚等模式,最大降低小说的传播),给予给作者合理的收入(因为涉及到网络小说的信息传播权,有待了解) 是否可以建立一套合理的小说推荐系统,推荐好的小说,促进圈子良好向上发展 之前写的小说评论爬虫(后续的推荐小说部分失败…),也是出于维护生态圈的目的,可惜困难较大,没有成功… 路漫漫其修远兮,吾将上下而求索.]]></content>
  </entry>
  <entry>
    <title><![CDATA[2_剑网3_交易信息爬虫(待完善)]]></title>
    <url>%2F2018%2F02%2F20%2F2-%E5%89%91%E7%BD%913-%E4%BA%A4%E6%98%93%E4%BF%A1%E6%81%AF%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[玩剑三许多年了,虽然已经AFK,但不时还是会接触到有关剑三的一些信息以前作为一名小小的pvg玩家,进行过各种交易,遇到过各式各样的玩家,其中也出现过些许不如人意的体验.由于剑三的交易金额往往较大,加之没有官方的交易系统,官方监管困难,其高额的收益与较低的成本导致交易的道德风险大大增加因此想要建个网站,提供一些有关剑网3的交易信息. update : 2018-2-21更改noises_id_list,从list更改为numpy.array()方式读取,节省内存 1.思路1).遍历贴吧(由服吧再在之后爬取更多有关贴吧)(目前只爬取了乾坤一掷吧)2).对贴吧首页(etc)的帖子标题及摘要,进行正则检索,出现触发词则对该帖首页进行严格触发词检索3).对满足条件的帖子进行遍历爬取4).继续遍历其他帖子5).保存相关数据至数据库,以及增量爬取所需信息至本地文件6).循环间断爬取增量内容7).对爬取的帖子内容进行文本处理(停留于此) 2.具体1).爬取的数据结构,即Items123456789101112131415class TiebaUrlItem(scrapy.Item): #用于增量爬取 cheater_url_dict = scrapy.Field() #&#123;url_id:[latest_page,latest_floor,reply_count]&#125;class TiebaNoisesItem(scrapy.Item): #用于去重 noises_id_list = scrapy.Field() #[url_id1,url_id2,url_id3...etc]class TiebaCheaterItem(scrapy.Item): cheater_url = scrapy.Field() #贴子url url_id = scrapy.Field() #贴子id post_title = scrapy.Field() #帖子标题 floor_num = scrapy.Field() #楼层数 user_id = scrapy.Field() #楼层id user_name=scrapy.Field() #楼层姓名 reply_content = scrapy.Field() #举报内容 image_url = scrapy.Field() #举报内容图片 #2).spider主要结构(非完整代码)12345678910111213141516171819202122232425262728293031323334class Tieba_CheaterSpider(CrawlSpider): pattern = re.compile(etc) start_urls = [&apos;https://tieba.baidu.com/f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&amp;ie=utf-8&apos;] rules=( Rule(LinkExtractor(allow=r&apos;f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&amp;ie=utf-8&amp;pn=\d+&apos;,restrict_xpaths=(&apos;.//div[@id=&quot;frs_list_pager&quot;]&apos;)),# callback=&apos;parse_index_url&apos;,follow=True), ) with open(&apos;noises_id_list.txt&apos;,&apos;r&apos;) as f: noises_id_list = f.readlines()[0].split(&apos;,&apos;) #去重列表 new_noises=[] #新噪点贴子id列表 try: with open(&apos;cheater_url_dict.json&apos;,&apos;r&apos;) as f: old_cheater_url_dict = json.load(f) except: old_cheater_url_dict = &#123;&#125; #增量url_id字典 def parse_index_url(self,response): #处理贴吧页面,爬取举报贴 bodys= for i in bodys: 判断url_id是否是noises_id_list 判断是否含有触发词 判断是否含有严格触发词 pri_judge 判断是否需要增量爬取 调用parse_all_detail yield TiebaUrlItem def parse_all_detail(): #爬取数据 meta传递数据 根据楼层信息与latest_floor对比, 大于则爬取,小于则不 yield TiebaCheaterItem 存在下一页 更新meta 并调用parse_all_detail() 不存在 调取meta 保存至TiebaUrlItem yield TiebaUrlItem def pri_judge(): 3).pipelines 以及 settingsa.pipelines 数据库处理,以及文件处理b.settings 一些设置 log_level, 数据库参数, 去重及增量list,dict 4).存在的问题a.下载中间件的使用,随机UA没有实现,在使用该中间件时,无法爬取网页b.meta的使用c.数据结构的理解 待增加的功能及 idea由于技术问题,目前只做到贴吧帖子的抓取,对自动识别骗子,并提取相关信息没有想到好的解决办法以下是一些展望1).增加评论的抓取(似乎不是很重要,没有深入)2).文本处理的完善,主动筛选出骗子信息(较难)3).SMTP 发送邮件 (似乎不重要,较为简单)4).查询后权限投票,增加查询者的便捷度5).根据投票结果,对数据库进行删改,提高准确性6).分布式优化,加快爬取速度 由于抓取了所有信息帖子信息,目前较为简单的做法是按照JX3YYYY.com作者的思路,由查询者提供关键词,返回相关信息(dict),由查询者自行判断是否是骗子]]></content>
  </entry>
  <entry>
    <title><![CDATA[1_随机UA与代理IP]]></title>
    <url>%2F2018%2F02%2F17%2F1-%E9%9A%8F%E6%9C%BAUA%E4%B8%8E%E4%BB%A3%E7%90%86IP%2F</url>
    <content type="text"><![CDATA[在抓取爬虫的过程中,往往会遇到各种反爬虫措施现记录下在网上学习到的关于UA和IP的方案 1.随机UA:部分网站会检查是否是浏览器访问,对于这种反爬虫策略加上User-Agent访问即可.在大量爬取的情况下,使用同一个UA是不够的. 常用方法如下:12345# -*- coding:utf-8 -*-filename = &apos;user-agents.txt&apos;with open(&apos;filename&apos;,&apos;r&apos;) as f: user-agents = f.readlines()user-agent = random.choice(user-agents) 2.代理IP当使用同一个IP大量快速访问某个页面时,网站服务器可能将你判定为爬虫此时可能会出现IP封禁或者出现验证码,输入正确才能继续访问 应对这种反爬虫措施,有以下几种解决方案:1).使用VPN,该方法简便,安全性高,实时性高,速度快,但价格较高.2).IP代理商,方法简便,安全性高,可用率大,速度快,稳定性好,但价格较高3).ASDL拨号,ASDL断开重连拨号后分配的IP地址会变化,免费,但效率低,实时性不高4).自建IP代理池,免费,方法较为简便,实时性一般,速度一般,适合个人学习使用. 自建IP代理池的原理及步骤:1).利用正则表达式,Xpath,CSS等爬取各个ip代理网站的ip地址2).将IP地址保存进数据库(sqlite3,mysql,mongodb,redis等)3).间断持续检测数据库中的ip有效性和速度,当有效IP少于一定数值,进行新一轮爬取4).设计API接口,提供从Pool中获取IP地址,及删除IP地址的接口,以及个性化方案 网上已有许多成熟的开源IP代理池项目本人的话,使用的是七夜的IPProxyPool项目([地址][])[地址]:https://github.com/qiyeboy/IPProxyPool 需要注意的是,随机UA和频繁更换IP有时并不一定有效,甚至是错误的行为实际爬取时,部分网站会请求重定向,或多重定向在这些过程中,在一个会话中更换UA或IP可能会导致出错.]]></content>
  </entry>
  <entry>
    <title><![CDATA[0_小说评论抓取]]></title>
    <url>%2F2018%2F02%2F16%2F0-%E5%B0%8F%E8%AF%B4%E8%AF%84%E8%AE%BA%E6%8A%93%E5%8F%96%2F</url>
    <content type="text"><![CDATA[看了这么多年小说,也想要写个小说推荐目前的想法是根据小说的评论数,收藏数,营养液,评论内容估计这本小说的推荐指数. 使用的库:pymongo, requests, re, jieba, BeautifulSoup, lxml.etree, multiprocessing.pool.Poll 初定步骤:1).抓取小说相关数据:评论数,收藏数,营养液数,评论内容2).将这些数据写入mongodb3).利用文本分析工具分析出小说评论的情感倾向 进度:完成了步骤1、2,在进行步骤3时,由于没有学习机器学习,在利用jieba将评论分词之后,想到的办法只有利用snownlp根据分词词性,对其赋分,感觉这样的推荐方式的可信度不高,故没有继续进行下去. 总结:目前只实现了根据Url多线程抓取评论,并写入mongodb数据库,在文本情感分析方面没有较好的进展由于时间问题,没有继续深入下去,暂保留于此 待完善的部分:大规模爬取的时候,ip代理的设置提高效率,分布式的应用文本情感分析,利用机器学习训练一个较好的模型,完成小说推荐的最后一个步骤]]></content>
  </entry>
</search>
