<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[8-淘宝爬虫]]></title>
    <url>%2F2018%2F03%2F11%2F8-%E6%B7%98%E5%AE%9D%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[春招开始了…心里慌得一批…不过还是要保持记录blog的习惯,这两天在研究淘宝的爬虫,之前觉得很简单,实际去写难爆了,目前只完成了淘宝部分,并且未进行优化,天猫商品部分有待研究.想着继续学习scrapy,并且利用scrapy的异步优势,就不用去学习…没想到大大增加了初步学习的难度.真的是纸上谈来终觉浅,绝知此事要躬行.以下是爬虫相关初步思路:因为是初学者,所以没想着爬取全站商品信息,就以商品关键词为展开,爬取淘宝/天猫商品.将所爬取的信息以dict方式保存入mongodb数据库 关于某种商品的所有信息,大概需要进行3次访问,第一次访问商品检索页,获取包含该关键字所有商品粗略信息(当页),其包括:12345678goods_id #商品idgoods_type #商品类型goods_title #商品名称goods_url = #商品链接goods_sales #付款人数goods_price #首页价格goods_shop #商家goods_addr #商家地区 第二次访问某一商品的具体详情页,可以获取以下所需信息:123goods_d_skuid #商品详细型号idgoods_d_title #商品详细型号标题goods_d_names #商品详细name 由于商品价格是ajax动态生成,所以需要进行第三次访问其api,获得某一商品具体型号的以下信息123goods_d_pre_price #商品详细原格goods_d_now_price #商品详细现价goods_d_quantity #商品库存 淘宝和天猫的信息获取方式基本一致,都是利用正则表达式匹配访问返回的响应,利用json库将其字典化,只是详细信息获取的api不同淘宝和天猫最大的不同,也是天猫最大的难点在于其cookie的需要(可能是其他原因),无论是访问其商品详情页,还是其价格api,都需要cookie然而,在进行一定次数的访问api后,应该是cookie失效,出现验证码页面,导致爬虫失败 待优化:优化商品名称检索,由每一个商品的具体型号title每次进行一次正则匹配data-value组合,改为提前进行data-value正则匹配,提取出一个集合,再根据商品的data-value组成商品具体型号title增加分布式爬取提高效率,建立cookie池,完善天猫爬取部分.在进行一定数量爬取后验证是否需要增加以下措施使用代理ip池,绑定cookie?使用随机ua.]]></content>
  </entry>
  <entry>
    <title><![CDATA[7_redis初步学习]]></title>
    <url>%2F2018%2F03%2F03%2F7-redis%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[…前几天回了学校,整理了下生活物品并玩了好久游戏…着实荒废了一阵,今天重拾学习,完成了初步学习redis的计划…真是惭愧. 以下是学习redis局域网部署的详细过程 1.绑定ip在尝试连接到局域网下的另一台电脑时,产生了无法连接的错误,仔细查阅了网上各种资料,终于在这个网页找到了对redis的conf文件中bind ip的较为详细的解释,解决了问题 即修改redis安装文件夹下的redis.windows.conf文件:12#修改bind 127.0.0.1 bind 192.168.0.* 127.0.0.1 (*为局域网IP) 2.测试连接另一台电脑连接另一台电脑1$ redis-cli -h 192.168.1.101 -p 6373 出现错误(error) NOAUTH Authentication required.则输入在conf文件中设置的密码1auth xxxxxx 测试是否通讯成功12345192.168.1.101:6379&gt; set name lrancok#另一台127.0.0.1:6379&gt; get name&apos;lranc&apos; 3.简单的分布式爬虫案例此案例根据TTyb的博客爬虫教程的第14章,之前由于虚拟机的问题,放弃了学习,今天苦苦搜寻了有关redis相关教程,发现还是该教程简单易懂,故重温.网址由于对redis的掌握不熟练,故将原blog的redis集群改为了redis单机思路与其blog一致,共有四个文件,insert.py与check.py用于验证redis通讯是否连接成功,并添加需要抓取的url.spider1.py与spider2.py的内容基本一致,只在将html写入文件时,保存的文件名处作出了区分.代码如下insert.py:1234567891011121314151617# -*- coding: utf-8 -*-#插入数据脚本import redis#连接redisredis_client = redis.Redis(host=&apos;192.168.1.101&apos;,port=6379,password=&apos;mima123427&apos;)redis_client.flushdb() #清空管道#增加url到redisdef pushToRedis(name,valuelist): for i in range(50): for item in valuelist: redis_client.lpush(name,item)name = &apos;url&apos;urlList = [&apos;https://www.baidu.com&apos;,&apos;http://www.tybai.com/&apos;]pushToRedis(name,urlList) check.py:1234567891011# -*- coding: utf-8 -*-#检查数据脚本import redis #连接redisredis_client = redis.Redis(host=&apos;192.168.1.101&apos;,port=6379,password=&apos;mima123427&apos;)name = &apos;url&apos;length = redis_client.llen(name)print(length)print(redis_client.lrange(name,0,-1)) spider1.py,spider2.py:123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-import requestsimport socketimport timeimport redissession = requests.session()headers = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.3; WOW64; rv:32.0) Gecko/20100101 Firefox/32.0&quot;&#125;redis_client = redis.Redis(host=&apos;192.168.1.101&apos;,port=6379,password=&apos;mima123427&apos;)#从redis中取出urldef pop_from_redis(name): return redis_client.rpop(name).decode()def get_html(url): #修饰头部 headers.update(dict(Referer=url)) #抓取页面 response = session.get(url=url,headers=headers) return response.content.decode(&apos;utf-8&apos;,&apos;ignore&apos;)#保持到本地def save_to_local(html): hostname = socket.gethostname() ip_name = (&quot;2&quot; + socket.gethostbyname(hostname) + &quot;#&quot; + str(time.time())).replace(&quot;.&quot;, &quot;_&quot;) #与spider1的不同点 with open(ip_name+&apos;.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;,errors=&apos;ignore&apos;) as f: f.write(html)def main(): name = &apos;url&apos; length = redis_client.llen(name) for i in range(length): url = pop_from_redis(name) print(url) html = get_html(url) save_to_local(html) time.sleep(1)if __name__ == &apos;__main__&apos;: time.sleep(5) main() 当出现AttributeError: ‘NoneType’ object has no attribute ‘decode’则说明redis消息集合中的url已经爬取完毕.可以在两台机子存放的文件夹中看到爬取的html文件,一台爬取了56页面,一台爬取了44个页面. 总结关于redis的初步学习就到此,下一步是继续学习redis在scrapy的应用,以及Django部署骗子查询]]></content>
  </entry>
  <entry>
    <title><![CDATA[6_5173爬虫优化]]></title>
    <url>%2F2018%2F02%2F25%2F6-5173%E7%88%AC%E8%99%AB%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[今天继续前一天写的爬虫,昨晚在爬取到240页的时候,还是因为Ip地址不够而抓取失败了,今天的任务是在删除数据库中的重复数据之后增量抓取.所以文章结构是:1.删除MongoDB中的重复数据2.布隆过滤器增量抓取3.redis初步学习 简单方法在今天的爬取过程中发现,商品的发布时间其实在url里,所以并不需要访问其详情页进去爬取()!!!错误其商品url时间为商品上架时间,而不是商品价格时间的具体表现,而这个爬虫的目的是为了知晓商品的价格走势,故没用 1.删除MongoDB中的重复数据:由于对MongoDB掌握不是很熟练,是在网上找到的方法.网址 该方法是先将数据导出为json,然后删除collection的数据,再新建唯一index,需要注意的是不能在删除重复数据后直接导入,虽然建立索引后插入数据,得到了一个每个url都唯一的collection,但是在之后的增量爬取时,因为index的升降序问题,导致有的数据无法插入,因此有2种方案(其实一样):a.在建立索引后,导入数据,得到一份无重复的数据,再导出,再remove,增量爬取,最后插入第二次导出的数据b.在建立索引后,导入数据,得到一份无重复的数据,新建一份collection,增量爬取,最后合并数据具体代码:1234567$ mongoexport -d 5173_jw3_price -c jw3_goods -o jw3_goods.json$ mongo$ use 5173_jw3_price #切换数据库$ db.jw3_goods.remove(&#123;&#125;) #删除数据$ db.jw3_goods.createIndex(&#123;goods_url:1&#125;,&#123;unique:true&#125;) #建立以goods_url唯一索引的文档$ exit# mongoimport -d 5173_jw3_price -c jw3_goods --upsert jw3_goods.json #插入数据 2.布隆过滤器增量抓取:对昨天的5173_spider.py文件进行修改,将以下部分以及逻辑判断加入123456789101112131415#首先将所有已抓取的goods_url导出:$ mongoexport -d 5173_jw3_price -c jw3_goods -f goods_url -o jw3_goods_url.json------import jsonimport sysfrom pybloom import ScalableBloomFilter #导入可动态扩展容量的ScalableBloomFilterwith open(&apos;jw3_goods_url.json&apos;,&apos;r&apos;) as f: url_list = [json.loads(x)[&apos;goods_url&apos;] for x in f.readlines()] #获取已抓取的url列表sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)for i in url_list: sbf.add(i)print(sys.getsizeof(sbf)) #56print(sys.getsizeof(url_list)) #61432 3.redis初步学习见下一篇…]]></content>
  </entry>
  <entry>
    <title><![CDATA[5_抓取5173商品信息]]></title>
    <url>%2F2018%2F02%2F24%2F5-%E6%8A%93%E5%8F%965173%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[今天这个爬虫真的是抓的我头都大飞了…初步看这个网站的数据,结构清晰层次合理,完美啊!…没想到有一个大坑坑坑坑坑… 本来继上一篇blog所想,是学习redis分布式的,结果一天都卡在IP代理池的应用…话不多说,首先确定我们需要的item结构,商品url,title,price,info,time,幸运的是5173网站商品结构清晰,从html中十分容易提取 可以看到,在商品列表中每一项商品栏可以提取item的前四项,而第五项,商品上架时间也可以通过提取的url访问新页面,再提取出去 这样计算大概需要369×40+369=15129次访问(14760个商品页+369个商品列表页),即可获取全部数据…然而事与愿违,昨晚将基本代码完成之后,爬取了一页…爬虫便倒下了…昨夜还想着,哇,终于遇到一个封IP的网站了,今天可以好好干一架,便喜滋滋地去睡觉了… 以下是总结今天遇到的几个难点:1).scrapy 使用proxy代理,完全没有用啊!!!肯定是我的使用方式不对!但是就是没找出原因,无论是百度还是知乎还是stackoverflow,都没有找到相应的解决方案,目前几个猜测点: scrapy的版本问题, python3的问题2).好,既然封IP,而自己用scrapy无法实现IP代理,那么就用scrapy.Request获取商品列表页,利用requests.get()获取商品详情页获取time,问题又来了,在不设置time.sleep()的情况下,依然一下子就全封了3).其实也是与上一项相关的,这就令人十分疑惑了,将近100个IP地址一下子全被封,是不是该网站反爬虫的方式不是IP,或者不止于IP?由于自己的知识掌握较少,在使用了随机UA之后,依然没能找出其封禁手段4).没办法,只好在利用requests.get()的同时多sleep下了…目前爬虫仍在运行,不过14760个item却要4个多小时左右完成,确实速度太慢了,而且还不能确保这种抓取方式能否抓完…等待简直煎熬… 改进及构想:1).利用redis分布式抓取,这也是这个项目原本最初的学习目的2).可遍历商品列表,将369×40个商品的url,title,price,info字典格式保存至json,再在之后利用字典中的url增量抓取time,以避免抓取失败,失去进度,顺便进一步学习多线程、多进程、异步以及布隆过滤器,最后保存数据库 最后,希望抓取成功…明天的任务是学习redis,以及mongo删除重复的数据]]></content>
  </entry>
  <entry>
    <title><![CDATA[4_Django初步学习]]></title>
    <url>%2F2018%2F02%2F23%2F4-Django%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[这两天初步学习了下django,照着文档完成了入门教程…学的粗糙,总的来说是一头雾水的… 本来是想着这两天上线一个初步能够使用的查询网站,应用于查询之前所爬取的百度贴吧的骗子信息(未筛选,优化)但没想到django的难度比较大,而马上又要开学找工作了,只好将时间与精力集中与爬虫的学习. 故而在完成文档的入门教程之后,只能在学习爬虫的闲暇之余尝试能不能照着网上可能存在的模板去实现需求 接下来几天的任务:1).爬取5173,争取使用分布式redis完成2).实现一个论坛的抓包分析,及布隆过滤器的增量抓取,先从简单的非scrapy架构3).实现应用scrapy,redis分布式,mongodb的集群以及布隆过滤器抓取,项目待构思4).在前两天,争取粗略实现web查询数据库应用(jw3_cheater_lookup) 加油~]]></content>
  </entry>
  <entry>
    <title><![CDATA[3_剑网3_交易信息网站构思]]></title>
    <url>%2F2018%2F02%2F21%2F3-%E5%89%91%E7%BD%913-%E4%BA%A4%E6%98%93%E4%BF%A1%E6%81%AF%E7%BD%91%E7%AB%99%E6%9E%84%E6%80%9D%2F</url>
    <content type="text"><![CDATA[继上一篇,接下来所要做的工作大体上是建立一个网站,当然,最终目标可不能仅仅提供一个查询骗子信息的接口,还要有其他几部分功能的!2333以下记录下一些想法 建站构思网站主要分为3大功能及几个小功能(目前)1).骗子查询接口,并在后期增加投票功能.2).商品行情信息(价格走势图),并以交易信息发布的多少(发布id判断),模拟其交易量,最终是建立一个期权期货交易平台233333).账号交易信息,交易平台??? 中介中介平台4).骗子举报接口5).评论区???(似乎不重要)6).还没有想出来… 展望嗯,就把建立一个骗子信息查询接口作为建站道路上的第二步吧(第一步是建立此博客hhh)之后的话则想为伟大的百合事业添砖加瓦了!!!这些年看过许多盗版小说也看过正版小说,双方的纠结点大概还是利益吧. 几个小想法:大多数看盗文的应该都追文? 大多数看的是最新章节以及最新完本. 或者说作者的大部分收入来源于正在连载的小说. 是否可以获取作者的同意,便宜出售早期小说,一方面为作者增加流量,并提高当期收益及期望收益,另一方面使得读者以较低的成本获得正版的心理体验以及阅读体验(对于小说以及与其发布网站的版权问题涉及签约合约规定,有待了解) 是否可以建立小说借阅模式的网站或APP(采用阅后即焚等模式,最大降低小说的传播),给予给作者合理的收入(因为涉及到网络小说的信息传播权,有待了解) 是否可以建立一套合理的小说推荐系统,推荐好的小说,促进圈子良好向上发展 之前写的小说评论爬虫(后续的推荐小说部分失败…),也是出于维护生态圈的目的,可惜困难较大,没有成功… 路漫漫其修远兮,吾将上下而求索.]]></content>
  </entry>
  <entry>
    <title><![CDATA[2_剑网3_交易信息爬虫(待完善)]]></title>
    <url>%2F2018%2F02%2F20%2F2-%E5%89%91%E7%BD%913-%E4%BA%A4%E6%98%93%E4%BF%A1%E6%81%AF%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[玩剑三许多年了,虽然已经AFK,但不时还是会接触到有关剑三的一些信息以前作为一名小小的pvg玩家,进行过各种交易,遇到过各式各样的玩家,其中也出现过些许不如人意的体验.由于剑三的交易金额往往较大,加之没有官方的交易系统,官方监管困难,其高额的收益与较低的成本导致交易的道德风险大大增加因此想要建个网站,提供一些有关剑网3的交易信息. update : 2018-2-21更改noises_id_list,从list更改为numpy.array()方式读取,节省内存 1.思路1).遍历贴吧(由服吧再在之后爬取更多有关贴吧)(目前只爬取了乾坤一掷吧)2).对贴吧首页(etc)的帖子标题及摘要,进行正则检索,出现触发词则对该帖首页进行严格触发词检索3).对满足条件的帖子进行遍历爬取4).继续遍历其他帖子5).保存相关数据至数据库,以及增量爬取所需信息至本地文件6).循环间断爬取增量内容7).对爬取的帖子内容进行文本处理(停留于此) 2.具体1).爬取的数据结构,即Items123456789101112131415class TiebaUrlItem(scrapy.Item): #用于增量爬取 cheater_url_dict = scrapy.Field() #&#123;url_id:[latest_page,latest_floor,reply_count]&#125;class TiebaNoisesItem(scrapy.Item): #用于去重 noises_id_list = scrapy.Field() #[url_id1,url_id2,url_id3...etc]class TiebaCheaterItem(scrapy.Item): cheater_url = scrapy.Field() #贴子url url_id = scrapy.Field() #贴子id post_title = scrapy.Field() #帖子标题 floor_num = scrapy.Field() #楼层数 user_id = scrapy.Field() #楼层id user_name=scrapy.Field() #楼层姓名 reply_content = scrapy.Field() #举报内容 image_url = scrapy.Field() #举报内容图片 #2).spider主要结构(非完整代码)12345678910111213141516171819202122232425262728293031323334class Tieba_CheaterSpider(CrawlSpider): pattern = re.compile(etc) start_urls = [&apos;https://tieba.baidu.com/f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&amp;ie=utf-8&apos;] rules=( Rule(LinkExtractor(allow=r&apos;f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&amp;ie=utf-8&amp;pn=\d+&apos;,restrict_xpaths=(&apos;.//div[@id=&quot;frs_list_pager&quot;]&apos;)),# callback=&apos;parse_index_url&apos;,follow=True), ) with open(&apos;noises_id_list.txt&apos;,&apos;r&apos;) as f: noises_id_list = f.readlines()[0].split(&apos;,&apos;) #去重列表 new_noises=[] #新噪点贴子id列表 try: with open(&apos;cheater_url_dict.json&apos;,&apos;r&apos;) as f: old_cheater_url_dict = json.load(f) except: old_cheater_url_dict = &#123;&#125; #增量url_id字典 def parse_index_url(self,response): #处理贴吧页面,爬取举报贴 bodys= for i in bodys: 判断url_id是否是noises_id_list 判断是否含有触发词 判断是否含有严格触发词 pri_judge 判断是否需要增量爬取 调用parse_all_detail yield TiebaUrlItem def parse_all_detail(): #爬取数据 meta传递数据 根据楼层信息与latest_floor对比, 大于则爬取,小于则不 yield TiebaCheaterItem 存在下一页 更新meta 并调用parse_all_detail() 不存在 调取meta 保存至TiebaUrlItem yield TiebaUrlItem def pri_judge(): 3).pipelines 以及 settingsa.pipelines 数据库处理,以及文件处理b.settings 一些设置 log_level, 数据库参数, 去重及增量list,dict 4).存在的问题a.下载中间件的使用,随机UA没有实现,在使用该中间件时,无法爬取网页b.meta的使用c.数据结构的理解 待增加的功能及 idea由于技术问题,目前只做到贴吧帖子的抓取,对自动识别骗子,并提取相关信息没有想到好的解决办法以下是一些展望1).增加评论的抓取(似乎不是很重要,没有深入)2).文本处理的完善,主动筛选出骗子信息(较难)3).SMTP 发送邮件 (似乎不重要,较为简单)4).查询后权限投票,增加查询者的便捷度5).根据投票结果,对数据库进行删改,提高准确性6).分布式优化,加快爬取速度 由于抓取了所有信息帖子信息,目前较为简单的做法是按照JX3YYYY.com作者的思路,由查询者提供关键词,返回相关信息(dict),由查询者自行判断是否是骗子]]></content>
  </entry>
  <entry>
    <title><![CDATA[1_随机UA与代理IP]]></title>
    <url>%2F2018%2F02%2F17%2F1-%E9%9A%8F%E6%9C%BAUA%E4%B8%8E%E4%BB%A3%E7%90%86IP%2F</url>
    <content type="text"><![CDATA[在抓取爬虫的过程中,往往会遇到各种反爬虫措施现记录下在网上学习到的关于UA和IP的方案 1.随机UA:部分网站会检查是否是浏览器访问,对于这种反爬虫策略加上User-Agent访问即可.在大量爬取的情况下,使用同一个UA是不够的. 常用方法如下:12345# -*- coding:utf-8 -*-filename = &apos;user-agents.txt&apos;with open(&apos;filename&apos;,&apos;r&apos;) as f: user-agents = f.readlines()user-agent = random.choice(user-agents) 2.代理IP当使用同一个IP大量快速访问某个页面时,网站服务器可能将你判定为爬虫此时可能会出现IP封禁或者出现验证码,输入正确才能继续访问 应对这种反爬虫措施,有以下几种解决方案:1).使用VPN,该方法简便,安全性高,实时性高,速度快,但价格较高.2).IP代理商,方法简便,安全性高,可用率大,速度快,稳定性好,但价格较高3).ASDL拨号,ASDL断开重连拨号后分配的IP地址会变化,免费,但效率低,实时性不高4).自建IP代理池,免费,方法较为简便,实时性一般,速度一般,适合个人学习使用. 自建IP代理池的原理及步骤:1).利用正则表达式,Xpath,CSS等爬取各个ip代理网站的ip地址2).将IP地址保存进数据库(sqlite3,mysql,mongodb,redis等)3).间断持续检测数据库中的ip有效性和速度,当有效IP少于一定数值,进行新一轮爬取4).设计API接口,提供从Pool中获取IP地址,及删除IP地址的接口,以及个性化方案 网上已有许多成熟的开源IP代理池项目本人的话,使用的是七夜的IPProxyPool项目(地址) 需要注意的是,随机UA和频繁更换IP有时并不一定有效,甚至是错误的行为实际爬取时,部分网站会请求重定向,或多重定向在这些过程中,在一个会话中更换UA或IP可能会导致出错.]]></content>
  </entry>
  <entry>
    <title><![CDATA[0_小说评论抓取]]></title>
    <url>%2F2018%2F02%2F16%2F0-%E5%B0%8F%E8%AF%B4%E8%AF%84%E8%AE%BA%E6%8A%93%E5%8F%96%2F</url>
    <content type="text"><![CDATA[看了这么多年小说,也想要写个小说推荐目前的想法是根据小说的评论数,收藏数,营养液,评论内容估计这本小说的推荐指数. 使用的库:pymongo, requests, re, jieba, BeautifulSoup, lxml.etree, multiprocessing.pool.Poll 初定步骤:1).抓取小说相关数据:评论数,收藏数,营养液数,评论内容2).将这些数据写入mongodb3).利用文本分析工具分析出小说评论的情感倾向 进度:完成了步骤1、2,在进行步骤3时,由于没有学习机器学习,在利用jieba将评论分词之后,想到的办法只有利用snownlp根据分词词性,对其赋分,感觉这样的推荐方式的可信度不高,故没有继续进行下去. 总结:目前只实现了根据Url多线程抓取评论,并写入mongodb数据库,在文本情感分析方面没有较好的进展由于时间问题,没有继续深入下去,暂保留于此 待完善的部分:大规模爬取的时候,ip代理的设置提高效率,分布式的应用文本情感分析,利用机器学习训练一个较好的模型,完成小说推荐的最后一个步骤]]></content>
  </entry>
</search>
